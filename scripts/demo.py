#!/usr/bin/env python
"""
Demo Script for Wellbeing Data Foundation

Demonstrates:
1. Ingestion results (from canonical JSON)
2. Evidence bundle retrieval
3. In-corpus question answering
4. Out-of-corpus refusal behavior
"""

import asyncio
import json
from pathlib import Path
from typing import Any, Optional

from apps.api.core.muhasibi_state_machine import create_middleware
from apps.api.retrieve.entity_resolver import EntityResolver
from apps.api.guardrails.citation_enforcer import create_guardrails


def print_header(text: str) -> None:
    """Print a section header."""
    print("\n" + "=" * 60)
    print(f"  {text}")
    print("=" * 60)


def print_json(data: Any, indent: int = 2) -> None:
    """Print JSON data nicely."""
    print(json.dumps(data, ensure_ascii=False, indent=indent))


def demo_sample_data() -> dict[str, Any]:
    """
    Create sample canonical data for demonstration.

    In production, this would come from the ingestion pipeline.
    """
    return {
        "meta": {
            "source_doc_id": "DOC_demo_001",
            "source_file_hash": "abcd1234efgh5678",
            "framework_version": "2025-10",
            "extracted_at": "2025-12-12T00:00:00",
            "stats": {
                "total_pillars": 1,
                "total_core_values": 2,
                "total_sub_values": 3,
                "total_evidence": 4,
            },
        },
        "pillars": [
            {
                "id": "P001",
                "name_ar": "Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ø§Ù„Ø·ÙŠØ¨Ø©",
                "description_ar": "Ø§Ù„Ø±ÙƒÙŠØ²Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† Ø±ÙƒØ§Ø¦Ø² Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø·ÙŠØ¨Ø©",
                "core_values": [
                    {
                        "id": "CV001",
                        "name_ar": "Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†",
                        "definition": {
                            "text_ar": "Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† Ù‡Ùˆ Ø§Ù„ØªØµØ¯ÙŠÙ‚ Ø¨Ø§Ù„Ù‚Ù„Ø¨ ÙˆØ§Ù„Ø¥Ù‚Ø±Ø§Ø± Ø¨Ø§Ù„Ù„Ø³Ø§Ù† ÙˆØ§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ù„Ø£Ø±ÙƒØ§Ù†",
                        },
                        "sub_values": [
                            {
                                "id": "SV001",
                                "name_ar": "Ø§Ù„ØªÙˆØ­ÙŠØ¯",
                                "definition": {
                                    "text_ar": "Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ù‡Ùˆ Ø¥ÙØ±Ø§Ø¯ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰ Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø¯Ø© ÙˆØ§Ù„Ø£Ù„ÙˆÙ‡ÙŠØ©",
                                },
                            },
                            {
                                "id": "SV002",
                                "name_ar": "Ø§Ù„Ø¥Ø®Ù„Ø§Øµ",
                                "definition": {
                                    "text_ar": "Ø§Ù„Ø¥Ø®Ù„Ø§Øµ Ù‡Ùˆ ØªØµÙÙŠØ© Ø§Ù„Ø¹Ù…Ù„ Ù…Ù† ÙƒÙ„ Ø´ÙˆØ§Ø¦Ø¨ Ø§Ù„Ø±ÙŠØ§Ø¡",
                                },
                            },
                        ],
                    },
                    {
                        "id": "CV002",
                        "name_ar": "Ø§Ù„ØµØ¨Ø±",
                        "definition": {
                            "text_ar": "Ø§Ù„ØµØ¨Ø± Ù‡Ùˆ Ø­Ø¨Ø³ Ø§Ù„Ù†ÙØ³ Ø¹Ù† Ø§Ù„Ø¬Ø²Ø¹ ÙˆØ§Ù„ØªØ³Ø®Ø·",
                        },
                        "evidence": [
                            {
                                "evidence_type": "quran",
                                "ref_raw": "[Ø§Ù„Ù†Ø­Ù„: 127]",
                                "text_ar": "ÙˆÙØ§ØµÙ’Ø¨ÙØ±Ù’ ÙˆÙÙ…ÙØ§ ØµÙØ¨Ù’Ø±ÙÙƒÙ Ø¥ÙÙ„ÙÙ‘Ø§ Ø¨ÙØ§Ù„Ù„ÙÙ‘Ù‡Ù",
                            },
                        ],
                        "sub_values": [
                            {
                                "id": "SV003",
                                "name_ar": "Ø§Ù„Ø±Ø¶Ø§",
                                "definition": {
                                    "text_ar": "Ø§Ù„Ø±Ø¶Ø§ Ù‡Ùˆ Ù‚Ø¨ÙˆÙ„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„Ù„Ù‡ ÙˆÙ‚Ø¯Ø±Ù‡ Ø¨Ø·ÙŠØ¨ Ù†ÙØ³",
                                },
                            },
                        ],
                    },
                ],
            },
        ],
    }


def demo_evidence_packets() -> list[dict[str, Any]]:
    """
    Create sample evidence packets for demonstration.
    """
    return [
        {
            "chunk_id": "CH_000001",
            "entity_type": "core_value",
            "entity_id": "CV001",
            "chunk_type": "definition",
            "text_ar": "Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† Ù‡Ùˆ Ø§Ù„ØªØµØ¯ÙŠÙ‚ Ø¨Ø§Ù„Ù‚Ù„Ø¨ ÙˆØ§Ù„Ø¥Ù‚Ø±Ø§Ø± Ø¨Ø§Ù„Ù„Ø³Ø§Ù† ÙˆØ§Ù„Ø¹Ù…Ù„ Ø¨Ø§Ù„Ø£Ø±ÙƒØ§Ù†",
            "source_doc_id": "DOC_demo_001",
            "source_anchor": "p10_abc123",
            "refs": [],
        },
        {
            "chunk_id": "CH_000002",
            "entity_type": "sub_value",
            "entity_id": "SV001",
            "chunk_type": "definition",
            "text_ar": "Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ù‡Ùˆ Ø¥ÙØ±Ø§Ø¯ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰ Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø¯Ø© ÙˆØ§Ù„Ø£Ù„ÙˆÙ‡ÙŠØ©",
            "source_doc_id": "DOC_demo_001",
            "source_anchor": "p15_def456",
            "refs": [],
        },
        {
            "chunk_id": "CH_000003",
            "entity_type": "core_value",
            "entity_id": "CV002",
            "chunk_type": "definition",
            "text_ar": "Ø§Ù„ØµØ¨Ø± Ù‡Ùˆ Ø­Ø¨Ø³ Ø§Ù„Ù†ÙØ³ Ø¹Ù† Ø§Ù„Ø¬Ø²Ø¹ ÙˆØ§Ù„ØªØ³Ø®Ø·",
            "source_doc_id": "DOC_demo_001",
            "source_anchor": "p20_ghi789",
            "refs": [],
        },
        {
            "chunk_id": "CH_000004",
            "entity_type": "core_value",
            "entity_id": "CV002",
            "chunk_type": "evidence",
            "text_ar": "ÙˆÙØ§ØµÙ’Ø¨ÙØ±Ù’ ÙˆÙÙ…ÙØ§ ØµÙØ¨Ù’Ø±ÙÙƒÙ Ø¥ÙÙ„ÙÙ‘Ø§ Ø¨ÙØ§Ù„Ù„ÙÙ‘Ù‡Ù",
            "source_doc_id": "DOC_demo_001",
            "source_anchor": "p21_jkl012",
            "refs": [{"type": "quran", "ref": "Ø§Ù„Ù†Ø­Ù„:127"}],
        },
    ]


async def demo_ingestion_results() -> None:
    """Demonstrate ingestion results."""
    print_header("1. Ingestion Results (Sample Data)")

    data = demo_sample_data()

    print("\nğŸ“Š Ingestion Statistics:")
    stats = data["meta"]["stats"]
    print(f"  â€¢ Pillars: {stats['total_pillars']}")
    print(f"  â€¢ Core Values: {stats['total_core_values']}")
    print(f"  â€¢ Sub-Values: {stats['total_sub_values']}")
    print(f"  â€¢ Evidence Items: {stats['total_evidence']}")

    print("\nğŸ“‹ Extracted Hierarchy:")
    for pillar in data["pillars"]:
        print(f"\n  ğŸ›ï¸ {pillar['name_ar']}")
        for cv in pillar["core_values"]:
            print(f"    â””â”€â”€ ğŸ’ {cv['name_ar']}")
            for sv in cv.get("sub_values", []):
                print(f"        â””â”€â”€ ğŸŒ± {sv['name_ar']}")


async def demo_evidence_bundle() -> None:
    """Demonstrate evidence bundle retrieval."""
    print_header("2. Evidence Bundle Retrieval")

    packets = demo_evidence_packets()

    print(f"\nğŸ“¦ Retrieved {len(packets)} evidence packets:\n")

    for i, packet in enumerate(packets, 1):
        print(f"  [{i}] Chunk ID: {packet['chunk_id']}")
        print(f"      Type: {packet['chunk_type']} ({packet['entity_type']})")
        print(f"      Text: {packet['text_ar'][:60]}...")
        if packet["refs"]:
            print(f"      Refs: {packet['refs']}")
        print()


async def demo_in_corpus_question() -> None:
    """Demonstrate in-corpus question answering."""
    print_header("3. In-Corpus Question: Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†ØŸ")

    # Set up resolver with sample data
    resolver = EntityResolver()
    resolver.load_entities(
        pillars=[{"id": "P001", "name_ar": "Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø±ÙˆØ­ÙŠØ© Ø§Ù„Ø·ÙŠØ¨Ø©"}],
        core_values=[
            {"id": "CV001", "name_ar": "Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†"},
            {"id": "CV002", "name_ar": "Ø§Ù„ØµØ¨Ø±"},
        ],
        sub_values=[
            {"id": "SV001", "name_ar": "Ø§Ù„ØªÙˆØ­ÙŠØ¯"},
            {"id": "SV002", "name_ar": "Ø§Ù„Ø¥Ø®Ù„Ø§Øµ"},
            {"id": "SV003", "name_ar": "Ø§Ù„Ø±Ø¶Ø§"},
        ],
    )

    middleware = create_middleware(entity_resolver=resolver)
    response = await middleware.process("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†ØŸ")

    print("\nğŸ” Query: Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¥ÙŠÙ…Ø§Ù†ØŸ")
    print(f"\nğŸ“ Listen Summary: {response.listen_summary_ar}")
    print(f"\nğŸ¯ Purpose: {response.purpose.ultimate_goal_ar}")
    print(f"\nğŸ“‹ Plan Steps:")
    for i, step in enumerate(response.path_plan_ar, 1):
        print(f"   {i}. {step}")

    print(f"\nğŸ’¬ Answer: {response.answer_ar}")
    print(f"\nğŸ“Š Confidence: {response.confidence.value}")
    print(f"â“ Not Found: {response.not_found}")

    if response.entities:
        print("\nğŸ·ï¸ Detected Entities:")
        for entity in response.entities:
            print(f"   â€¢ {entity.name_ar} ({entity.type.value})")


async def demo_out_of_corpus_refusal() -> None:
    """Demonstrate out-of-corpus refusal."""
    print_header("4. Out-of-Corpus Question (Refusal Demo)")

    middleware = create_middleware()

    # Question clearly outside the wellbeing framework
    question = "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© ÙØ±Ù†Ø³Ø§ØŸ"

    print(f"\nğŸ” Query: {question}")
    print("   (This is outside the wellbeing framework scope)")

    response = await middleware.process(question)

    print(f"\nğŸ’¬ Answer: {response.answer_ar}")
    print(f"\nâ“ Not Found: {response.not_found}")
    print(f"ğŸ“Š Confidence: {response.confidence.value}")
    print(f"ğŸ“š Citations: {len(response.citations)}")

    print("\nâœ… Refusal behavior confirmed: System refuses to hallucinate!")


async def demo_guardrails() -> None:
    """Demonstrate guardrails validation."""
    print_header("5. Guardrails Validation Demo")

    guardrails = create_guardrails(min_coverage_ratio=0.5)
    packets = demo_evidence_packets()

    # Valid answer with citations
    print("\nâœ… Test 1: Valid answer with proper citations")
    result = guardrails.validate(
        answer_ar="Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† Ù‡Ùˆ Ø§Ù„ØªØµØ¯ÙŠÙ‚ Ø¨Ø§Ù„Ù‚Ù„Ø¨ ÙˆØ§Ù„Ø¥Ù‚Ø±Ø§Ø± Ø¨Ø§Ù„Ù„Ø³Ø§Ù†",
        citations=[{"chunk_id": "CH_000001"}],
        evidence_packets=packets,
        not_found=False,
    )
    print(f"   Passed: {result.passed}")
    print(f"   Issues: {result.issues or 'None'}")

    # Invalid: no citations
    print("\nâŒ Test 2: Answer without citations (should fail)")
    result = guardrails.validate(
        answer_ar="Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† Ù‡Ùˆ Ø§Ù„ØªØµØ¯ÙŠÙ‚ Ø¨Ø§Ù„Ù‚Ù„Ø¨",
        citations=[],
        evidence_packets=packets,
        not_found=False,
    )
    print(f"   Passed: {result.passed}")
    print(f"   Issues: {result.issues}")
    print(f"   Should Retry: {result.should_retry}")

    # Invalid: invalid chunk_id
    print("\nâŒ Test 3: Citation with invalid chunk_id (should fail)")
    result = guardrails.validate(
        answer_ar="Ø§Ù„Ø¥ÙŠÙ…Ø§Ù† Ù‡Ùˆ Ø§Ù„ØªØµØ¯ÙŠÙ‚ Ø¨Ø§Ù„Ù‚Ù„Ø¨",
        citations=[{"chunk_id": "CH_INVALID"}],
        evidence_packets=packets,
        not_found=False,
    )
    print(f"   Passed: {result.passed}")
    print(f"   Issues: {result.issues}")


async def main() -> None:
    """Run all demos."""
    print("\n" + "ğŸŒŸ" * 30)
    print("  WELLBEING DATA FOUNDATION - DEMO")
    print("  Evidence-Only Arabic Assistant")
    print("ğŸŒŸ" * 30)

    await demo_ingestion_results()
    await demo_evidence_bundle()
    await demo_in_corpus_question()
    await demo_out_of_corpus_refusal()
    await demo_guardrails()

    print("\n" + "=" * 60)
    print("  DEMO COMPLETE")
    print("=" * 60)
    print("\nâœ… All demos executed successfully!")
    print("ğŸ“Œ Key takeaways:")
    print("   â€¢ System extracts structured data from Arabic documents")
    print("   â€¢ Evidence packets are citeable with stable anchors")
    print("   â€¢ Muá¸¥ÄsibÄ« middleware enforces evidence-only answers")
    print("   â€¢ Guardrails block hallucination attempts")
    print("   â€¢ Out-of-scope questions trigger refusal (not_found=true)")
    print()


if __name__ == "__main__":
    asyncio.run(main())

